{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "# Script to evaluate citation delay\n",
    "# citation_id - patent making a citation \n",
    "# patent_id - patent receiving a citation \n",
    "\n",
    "# Renato Kogeyama\n",
    "\n",
    "\n",
    "# Oct 22, 2020\n",
    "# The original script requires more than 32 GB RAM\n",
    "# Changing from pd to dd (dask dataframe)\n",
    "\n",
    "# Aug 19, 2020\n",
    "# Included gzip\n",
    "# Run with latest database\n",
    "\n",
    "\n",
    "# Feb 07, 2020\n",
    "# The main offensor of performance in this script is the transformation to timedelta\n",
    "# the solution is to change to numpy\n",
    "# https://stackoverflow.com/questions/52274356/conversion-of-a-timedelta-to-int-very-slow-in-python\n",
    "\n",
    "# Jan 17 2020\n",
    "# Join cit_delay with var_builder\n",
    "# The only thing var_builder was doing was including kind and type \n",
    "\n",
    "\n",
    "# Jan 03 2020\n",
    "# Miami\n",
    "# I am using this script to calculate the average delay in citation - to follow Hall et al, 2001\n",
    "# patent.csv has the following columns\n",
    "# id \ttype \tnumber \tcountry \tdate \tabstract \ttitle \tkind \tnum_claims \tfilename\n",
    "# interest on id, type, date, kind, num_claims\n",
    "\n",
    "# I use two sources, uspatentcitation.tsv and patent.csv\n",
    "# The first is a citation-level dataset with information about the citing patent\n",
    "# The second is a patent-level dataset with information about the patent\n",
    "\n",
    "# Cleaning\n",
    "# I tested in other scripts the quality of the patent identifier\n",
    "# It does not require cleaning - only 4 erros from 6 million patents\n",
    "# The cleaning script is there anyway\n",
    "\n",
    "# Merging\n",
    "# I merge on the citation level (df)\n",
    "\n",
    "\n",
    "# --\n",
    "\n",
    "# First U.S. Patent Issued Today in 1790\n",
    "\n",
    "\n",
    "# July 31, 2001\n",
    "# Press Release\n",
    "# #01-33\n",
    "\n",
    "# On July 31, 1790 Samuel Hopkins was issued the first patent for a process \n",
    "# of making potash, an ingredient used in fertilizer. The patent was signed by \n",
    "# President George Washington. Hopkins was born in Vermont, but was living in \n",
    "# Philadelphia, Pa. when the patent was granted.\n",
    "\n",
    "# The first patent, as well as the more than 6 million patents issued since then, \n",
    "# can be seen on the Department of Commerce's United States Patent and Trademark \n",
    "# Office website at www.uspto.gov. The original document is in the collections of \n",
    "# the Chicago Historical Society.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "#import matplotlib.pyplot as plt\n",
    "import gzip\n",
    "import dask.dataframe as dd\n",
    "from dask.delayed import delayed\n",
    "import datetime\n",
    "import glob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_and_subtract_dates(df):\n",
    "    #conversao de string para data\n",
    "    df[\"citation_date\"] = df[\"citation_date\"].astype(int)\n",
    "    df[\"patent_date\"] = df[\"patent_date\"].astype(int)\n",
    "\n",
    "    # delay is the time interval between grant and citation\n",
    "    # following https://stackoverflow.com/questions/55395387/converting-a-dask-column-to-a-date-and-applying-a-lambda-function?rq=1\n",
    "    df=df.assign(cit_delay=df[\"citation_date\"] - df[\"patent_date\"])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_report(df, report, report_dst):\n",
    "    # if I do not drop nans, the script raises an error later when converting day interval into years\n",
    "    # I could substitute with average instead of dropping, this way I do not lose the citation info\n",
    "    # however, not always it will be possible to average - cases where there is ony one citation, for example\n",
    "    # For this reason, at this point, I'll keep the NAN and circumvent the issues as they arise\n",
    "\n",
    "    # df=df.dropna()\n",
    "\n",
    "    report.append(\"largest citation delays\\n\")\n",
    "    report.append(df.nlargest(15, 'cit_delay').to_latex())\n",
    "    report.append(\"smallest citation delays \\n\")\n",
    "    report.append(df.nsmallest(15, 'cit_delay').to_latex())\n",
    "\n",
    "    report.append(\"describe\\n\")\n",
    "    report.append(df.describe().to_latex())\n",
    "\n",
    "    report.append(\"head\\n\")\n",
    "    report.append(df.head().to_latex())\n",
    "\n",
    "    #get_ipython().run_cell_magic('time', '', 'df.hist()')\n",
    "\n",
    "    #Check outliers\n",
    "    #report.append(\"Check cit delay outliers - 0.15 quantile\")\n",
    "    #report.append(df[df[\"cit_delay\"]>df[\"cit_delay\"].quantile(0.15)].sort_values(by=['cit_delay'], ascending=True))\n",
    "\n",
    "    #report.append(\"Check cit delay outliers -0.85 quantile\")\n",
    "    #report.append(df[df[\"cit_delay\"]<df[\"cit_delay\"].quantile(0.85)].sort_values(by=['cit_delay'], ascending=False))\n",
    "    with open(report_dst, 'a') as f:\n",
    "        f.write(\"VAR BUILDER\\n\"+str(datetime.datetime.now()) + \"\\n\")\n",
    "        f.writelines([str(x) + \"\\n\" for x in report])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "patent= 'data/cleanpatent.parquet.gz'\n",
    "dst='data/var_builder'\n",
    "report_dst='var_builder_report.tex'\n",
    "\n",
    "report=[] #file to export report\n",
    "\n",
    "file_list=glob.glob(\"data/citation/*\")\n",
    "\n",
    "dfs = [delayed(pd.read_parquet)(f) for f in file_list]\n",
    "\n",
    "pt_df = dd.read_parquet(patent)\n",
    "\n",
    "report.append(\"VAR BUILDER \\n\")\n",
    "report.append(\"patent file head \\n\")\n",
    "report.append(pt_df.head().to_latex())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data/citation/clean_0.parquet.gz',\n",
       " 'data/citation/clean_8.parquet.gz',\n",
       " 'data/citation/clean_4.parquet.gz',\n",
       " 'data/citation/clean_3.parquet.gz',\n",
       " 'data/citation/clean_10.parquet.gz',\n",
       " 'data/citation/clean_1.parquet.gz',\n",
       " 'data/citation/clean_2.parquet.gz',\n",
       " 'data/citation/clean_6.parquet.gz',\n",
       " 'data/citation/clean_9.parquet.gz',\n",
       " 'data/citation/clean_5.parquet.gz',\n",
       " 'data/citation/clean_7.parquet.gz']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,df in enumerate(dfs):\n",
    "    \n",
    "    #df = dd.read_parquet(df)\n",
    "\n",
    "    report.append(\"file \"+ str(i) +\" citation head \\n\")\n",
    "    report.append(df.head().to_latex())\n",
    "\n",
    "    df=df.rename(columns = {'date':'patent_date'})\n",
    "    pt_df=pt_df.rename(columns = {'date':'citation_date'})\n",
    "\n",
    "    # merge between patent data and citations on patent_id (citing)\n",
    "    # merging on the citation dataset drops patents without citing\n",
    "    # later i could standardize to make patent_id index and use join instead of merge\n",
    "    df=df.merge(pt_df, how='inner', left_index=True, right_index=True)\n",
    "\n",
    "    # report.append(\"Info after merging\\n\")\n",
    "    # report.append(df.info().to_latex())\n",
    "\n",
    "    df=delayed(convert_and_subtract_dates)(df)\n",
    "    df=df.compute()\n",
    "    filename=dst+str(i)+\".parquet.gz\"\n",
    "    df.to_parquet(filename, compression=\"gzip\")\n",
    "    write_report(df, report, report_dst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
