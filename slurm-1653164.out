/bin/bash: error importing function definition for `BASH_FUNC_module()'
/bin/bash: error importing function definition for `BASH_FUNC_switchml()'
/bin/bash: error importing function definition for `BASH_FUNC__moduleraw()'
/home/rkogeyam/PATENT_CITATION
n056
Sun Aug 23 20:42:49 EDT 2020
Running program on 3 CPU cores
[NbConvertApp] Searching ['/home/rkogeyam/PATENT_CITATION', '/home/rkogeyam/.jupyter', '/home/applications/anaconda3/4.2.0/etc/jupyter', '/usr/local/etc/jupyter', '/etc/jupyter'] for config files
[NbConvertApp] Looking for jupyter_config in /etc/jupyter
[NbConvertApp] Looking for jupyter_config in /usr/local/etc/jupyter
[NbConvertApp] Looking for jupyter_config in /home/applications/anaconda3/4.2.0/etc/jupyter
[NbConvertApp] Looking for jupyter_config in /home/rkogeyam/.jupyter
[NbConvertApp] Looking for jupyter_config in /home/rkogeyam/PATENT_CITATION
[NbConvertApp] Looking for jupyter_nbconvert_config in /etc/jupyter
[NbConvertApp] Looking for jupyter_nbconvert_config in /usr/local/etc/jupyter
[NbConvertApp] Looking for jupyter_nbconvert_config in /home/applications/anaconda3/4.2.0/etc/jupyter
[NbConvertApp] Looking for jupyter_nbconvert_config in /home/rkogeyam/.jupyter
[NbConvertApp] Looking for jupyter_nbconvert_config in /home/rkogeyam/PATENT_CITATION
[NbConvertApp] Converting notebook external_knowledge.ipynb to notebook
[NbConvertApp] Notebook name is 'external_knowledge'
[NbConvertApp] Applying preprocessor: ExecutePreprocessor
[NbConvertApp] Executing notebook with kernel: python3
[NbConvertApp] Found kernel m-env in /home/rkogeyam/.local/share/jupyter/kernels
[NbConvertApp] Native kernel (python3) available from /home/applications/anaconda3/4.2.0/lib/python3.5/site-packages/ipykernel/resources
[NbConvertApp] Starting kernel: ['/home/applications/anaconda3/4.2.0/bin/python', '-m', 'ipykernel', '-f', '/tmp/tmpbkvpy_ii.json']
[NbConvertApp] Connecting to: tcp://127.0.0.1:35326
[NbConvertApp] connecting shell channel to tcp://127.0.0.1:39100
[NbConvertApp] Connecting to: tcp://127.0.0.1:39100
[NbConvertApp] connecting iopub channel to tcp://127.0.0.1:49956
[NbConvertApp] Connecting to: tcp://127.0.0.1:49956
[NbConvertApp] connecting stdin channel to tcp://127.0.0.1:46121
[NbConvertApp] Connecting to: tcp://127.0.0.1:46121
[NbConvertApp] connecting heartbeat channel to tcp://127.0.0.1:49206
[NbConvertApp] Executing cell:
#Script to evaluate the determinants of patent citation 
#Renato Kogeyama

# August 19, 2020
# Updated all files to latest data available in PatentsView (Jun/2020)
# Excluded self citations from the dataset

# July 13, 2020
# I am changing the script to test Nemet and Johnson 2012, but with centrality measures as DV

# Apr 27, 2020
# Separe most central patents, classify them as disruptive and calculate dvs from extant literature

# Mar 16, 2020
# Introducing centrality measures as dv

# Feb 04, 2020
# to set labels in heatmap keyword:xticklabels
# for ex.
# sns.heatmap(globalWarming_df, xticklabels = np.arange(0,15))
# to be implemented later
# another alternative is to substitute the values in the dataset and convert columns to categories
# to understand the impact, i should run some test
# however i am focusing now in calculate Corredoira's 2015 and Nemet & Johnson 2012

# Feb 03, 2020
# version backed up as _old

# Feb 02, 2020
# the best way to deal with the classification names is to use a dictionary
# this avoid charging memory with the strings
# However, WIPO is organized differently than the other systems
# I'll update the wipo code to uniformize the behavior in this script
# I am creating a code that reflects the first level of classification 

# Feb 01, 2020
# Introduction of categorical graphs: barplot and heatmap
# heatmap is not the real deal, its a simplification
# the real deal would be the correlation table - there is a suggestion based on cramer, 
    # but implementation was not ready
# graphs exported and google docs updated
# next step: update cit_tree to reflect Corredoira's 205 Influence measure
# plot a network graph: https://plot.ly/python/network-graphs/
# reproduce 2012 Nemet and Johnson with other class systems
# correct bias in generality and originality (multiply for N/N-1)

# Jan 21, 2020
# Classifications added
# Code reorganized - much faster now
# Still missing the update of applications to the grant number
# I should provide now descriptive statistics on all variables

# Jan 21, 2020
# The current data does not have Class
# I should go back and get this info - but there are too many scripts now and
#   I should reorganize them before moving forward
# I should also include the patent publication date - to control for the policy changes
# In the citation file, I should change application number for grant when possible 
#   This will improve realiability of all measures related to citation
# Introduce classifications

# Jan 18, 2020
# Variables calculated
# Generality, average delay, forward and backward citations, cumulative citation (cit_tree)
# Still missing originality
# the file with variables that are used in this script should get a name independent from the date


#Miami, December 24th, 2019
# Prof. Rafael Corredoira suggested:
# - Inclusion of a tree of citations
#   To track back the source of citations. This is information is not given by direct count of citations.
# - Consider policy changes in the way patents are cited
#   Policy changes in 2000 changed the time frame of citation, and 2010 partially moved citation to applications
# - Track classification changes 
#   The original classification system in USPTO changed from a technical based to a market based classification system
#   See if there is an impact
# - Consider a text analysis of the claims
#   Classification is based on the claims but it is not clear how many claims are related to each classification category
# - Include moderation effect from classification
#   Citations patterns may change across industries, so some effects may disappear if industry is not accounted for.

# In summary, his ideas help increase structure of the current work


#Syracuse, December 3rd, 2019

#The original script is getting too complex
#There was many tentative scripts to play with data
#Here I am writing a script to show the relevance of variables to patent citation

#11-12-2019
#Introducing normalization

#10-11-2019
#I introduced log backward citation, what corrects for very dispersed results
#but the major problem is that few patents receive citations
#bring back binary output

#10-10-2019
#Added graphics and new distributions

#10-03-2019
#I rewrote the citation data to clean the strings

#09-15-2019
#O naive bayes tem algum problema com distribuicoes desbalanceadas
#o scikit learn tem um modulo que corrige count distributions com muitos zeros, o complementNB
#porem este nao esta disponivel na atual versao disponibilizada no HPC da FIU

#09-10-2019
#o trabalho pede uma abordagem mais sistematica e cuidadosa
#estou agrupando o codigo antigo comentado e vou comecar um novo codigo

#09-27-2019
#I am renaming citation as forward citation and backward citation

#09-17-2018

#Alto uso de memoria - rodar no Amazon AWS 


[NbConvertApp] output: status
[NbConvertApp] output: execute_input
[NbConvertApp] output: status
[NbConvertApp] Executing cell:
import pandas as pd
import numpy as np
import IPython.display as display
import seaborn as sns
          
import itertools

from sklearn import preprocessing
from sklearn import linear_model, datasets
from sklearn.model_selection import train_test_split
from sklearn.feature_selection import SelectKBest, f_classif
from sklearn import naive_bayes
from sklearn.metrics import roc_curve, auc
from sklearn.feature_selection import VarianceThreshold
from sklearn.linear_model import LinearRegression

import scipy.stats as ss
import datetime
import matplotlib.pyplot as plt

from math import sqrt

import sys
sys.path.append('/home/rkogeyam/scripts/')
sys.path.append('scripts/')

#from determinants_scripts import classes

# from plotbar import plotbar
# from plot_heat import heatmap


from best_num_attr import best_num_attr
from xattrSelect import xattrSelect
from sampler import sampler
from normalize import normalize
from nbayes import nbayes

import gzip
import statsmodels.api as sm

import os

[NbConvertApp] output: status
[NbConvertApp] output: execute_input
[NbConvertApp] output: status
[NbConvertApp] Executing cell:
latex='data/results.tex'
# dataset='data/dataset.csv'
dataset=gzip.open('data/dataset.csv.gz', 'rt')
[NbConvertApp] output: status
[NbConvertApp] output: execute_input
[NbConvertApp] output: status
[NbConvertApp] Executing cell:
%matplotlib inline
sns.set()
sns.set_palette(sns.cubehelix_palette(8))
# pd.options.display.float_format = '{:,.2f}'.format
[NbConvertApp] output: status
[NbConvertApp] output: execute_input
[NbConvertApp] output: status
[NbConvertApp] Executing cell:
dtypes={'id':object,'type':object, 'kind':object, 'num_claims':float, 'cit_received':float, 'cit_made':float,
       'cit_received_delay':float, 'cit_made_delay':float, 'parent_citation':float,
       'originality':float, 'generality':float, 'wipo_sector_id':object, 'ipcr_section':object,
       'ipcr_ipc_class':object, 'ipcr_subclass':object, 'cpc_section_id':object,
       'cpc_subsection_id':object, 'cpc_group_id':object, 'nber_category_id':object,
       'nber_subcategory_id':object, 'uspc_mainclass_id':object, 'uspc_subclass_id':object, 'eigen':float, 'pagerank':float, 'katz':float}
[NbConvertApp] output: status
[NbConvertApp] output: execute_input
[NbConvertApp] output: status
[NbConvertApp] Executing cell:
# only main classes (exclude uspc)

usecols=['id', 'date', 'num_claims', 'cit_received', 'cit_made',
         'cit_received_delay', 'cit_made_delay',
         'originality', 'generality', 'wipo_sector_id', 'pagerank']
[NbConvertApp] output: status
[NbConvertApp] output: execute_input
[NbConvertApp] output: status
[NbConvertApp] Executing cell:
# only WIPO class system, exclude type and kind

# usecols=['id', 'date', 'num_claims', 'cit_received', 'cit_made',
#         'cit_received_delay', 'cit_made_delay', 'parent_citation',
#         'originality', 'generality', 'wipo_sector_id', 'eigen', 'pagerank', 'katz']
[NbConvertApp] output: status
[NbConvertApp] output: execute_input
[NbConvertApp] output: status
[NbConvertApp] Executing cell:
df=pd.read_csv(dataset, usecols=usecols, dtype=dtypes, parse_dates=['date'], index_col='id')

df.info()
[NbConvertApp] output: status
[NbConvertApp] output: execute_input
[NbConvertApp] output: stream
[NbConvertApp] output: status
[NbConvertApp] Executing cell:
df['year']=df.date.dt.year

df['decade']=df.date.dt.year//10*10
df['decade'] =df['decade'].apply(lambda x: int(x) if str(x) != 'nan' else np.nan)
decades=list(df.decade.unique())
# decades = [int(x) for x in decades if str(x) != 'nan']
[NbConvertApp] output: status
[NbConvertApp] output: execute_input
[NbConvertApp] output: status
[NbConvertApp] Executing cell:
obj_cols=list(df.select_dtypes(include=[object]).columns.values)
obj_cols
[NbConvertApp] output: status
[NbConvertApp] output: execute_input
[NbConvertApp] output: execute_result
[NbConvertApp] output: status
[NbConvertApp] Executing cell:
num_cols=list(df.select_dtypes(include=[np.number]).columns.values)
num_cols
[NbConvertApp] output: status
[NbConvertApp] output: execute_input
[NbConvertApp] output: execute_result
[NbConvertApp] output: status
[NbConvertApp] Executing cell:
descriptive=df.describe(include=[np.number]).loc[['count','mean','std','min','max']].append(df[num_cols].isnull().sum().rename('isnull'))
[NbConvertApp] output: status
[NbConvertApp] output: execute_input
[NbConvertApp] output: stream
[NbConvertApp] output: status
[NbConvertApp] Executing cell:
descriptive.apply(lambda x: x.apply('{:,.2f}'.format)).transpose()
[NbConvertApp] output: status
[NbConvertApp] output: execute_input
[NbConvertApp] output: execute_result
[NbConvertApp] output: status
[NbConvertApp] Executing cell:
df.describe(include=[np.object])#.append(df[np.object].isnull().sum().rename('isnull')).transpose()
[NbConvertApp] output: status
[NbConvertApp] output: execute_input
[NbConvertApp] output: execute_result
[NbConvertApp] output: status
[NbConvertApp] Executing cell:
# # barplot
# # as of 02.03.20, working

# for i in obj_cols:
#     plotbar(i, df, classes)

# # barplot with decades
# for i in obj_cols:
#     plotbar(i, df, classes,decade=True)

# # barplot with decades and inverted axis
# for i in obj_cols:
#     plotbar(i, df, classes,decade=True, decade_x=True)

# # heatmaps all periods
# for double in list(itertools.combinations(obj_cols, 2)):
#     heatmap(df[double[0]], df[double[1]]) 

# # print heatmaps per decade
# for decade in decades:
#     df_dec=df[df['decade']==decade]
#     for double in list(itertools.combinations(obj_cols, 2)):
#         heatmap(df_dec[double[0]], df_dec[double[1]], decade) 

[NbConvertApp] output: status
[NbConvertApp] output: execute_input
[NbConvertApp] output: status
[NbConvertApp] Executing cell:
# #histograms
# #could improve cutting off outliers
# for variable in num_cols:
#     ax=df[variable].hist()
#     ax.set_title('Histogram '+ variable.title()+'\n')
#     plt.show()
[NbConvertApp] output: status
[NbConvertApp] output: execute_input
[NbConvertApp] output: status
[NbConvertApp] Executing cell:
#iterate over numerical variables

num_cols.remove('decade')
num_cols.remove('year')
[NbConvertApp] output: status
[NbConvertApp] output: execute_input
[NbConvertApp] output: status
[NbConvertApp] Executing cell:
# for variable in num_cols:
    
#     title=variable.replace('_', ' ')
#     fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(14, 6))

#     axes[0] = df.groupby('year').mean().plot(y=variable, ax=axes[0])
#     evl_title='Evolution of '+ title +'\n'
#     axes[0].set_title(evl_title)
#     axes[0].set_ylim(bottom=0)
    
#     axes[1] = sns.boxplot(x='decade', y=variable, data=df)

#     box_title='Dispersion of '+ title +'\n'
#     axes[1].set_title(box_title)
#     axes[1].set_ylim(bottom=0)
#     axes[1].set_ylabel("")
    
#     filename='./img/evol_dispersion_'+variable.lower()+'.png'  
#     plt.savefig(filename) 
#     plt.show()

[NbConvertApp] output: status
[NbConvertApp] output: execute_input
[NbConvertApp] output: status
[NbConvertApp] Executing cell:
# the generality data on the 2010's is too concentrated around 0
# to check, I draw this hist to understand what is happening
# it could be an effect of truncation - generality increases with forward citation

# df[df['decade']==2010]['generality'].hist()
[NbConvertApp] output: status
[NbConvertApp] output: execute_input
[NbConvertApp] output: status
[NbConvertApp] Executing cell:
df.info()
[NbConvertApp] output: status
[NbConvertApp] output: execute_input
[NbConvertApp] output: stream
[NbConvertApp] output: status
[NbConvertApp] Executing cell:
# normalization
df=normalize(df.dropna())
[NbConvertApp] output: status
[NbConvertApp] output: execute_input
[NbConvertApp] output: stream
[NbConvertApp] output: stream
[NbConvertApp] output: stream
[NbConvertApp] output: stream
[NbConvertApp] output: status
[NbConvertApp] Executing cell:
df.info()
[NbConvertApp] output: status
[NbConvertApp] output: execute_input
[NbConvertApp] output: stream
[NbConvertApp] output: status
[NbConvertApp] Executing cell:
df.head()
[NbConvertApp] output: status
[NbConvertApp] output: execute_input
[NbConvertApp] output: execute_result
[NbConvertApp] output: status
[NbConvertApp] Executing cell:
# #maybe nb fit does not accept nomalized data, so i using data without normalize
# #but in that case, i have to transform the categorical variables

# obj_cols=list(df.select_dtypes(include=[object]).columns.values)

# for col in obj_cols:
#     df[col] = df[col].astype('category')

# df=pd.get_dummies(df, columns=obj_cols, prefix=obj_cols)
[NbConvertApp] output: status
[NbConvertApp] output: execute_input
[NbConvertApp] output: status
[NbConvertApp] Executing cell:
# List of IVs
chosenColumns=df.columns.values.tolist()
len(chosenColumns)
[NbConvertApp] output: status
[NbConvertApp] output: execute_input
[NbConvertApp] output: execute_result
[NbConvertApp] output: status
[NbConvertApp] Executing cell:
chosenColumns.remove('pagerank')
chosenColumns.remove('date')
chosenColumns.remove('wipo_sector_id')
len(chosenColumns)
[NbConvertApp] output: status
[NbConvertApp] output: execute_input
[NbConvertApp] output: execute_result
[NbConvertApp] output: status
[NbConvertApp] Executing cell:
wipo_sectors=df.wipo_sector_id.unique()
[NbConvertApp] output: status
[NbConvertApp] output: execute_input
[NbConvertApp] output: status
[NbConvertApp] Executing cell:
df[df.wipo_sector_id=='1'].head()
[NbConvertApp] output: status
[NbConvertApp] output: execute_input
[NbConvertApp] output: execute_result
[NbConvertApp] output: status
[NbConvertApp] Executing cell:
#function to write results to a latex file 
def export_table(content, name):
    basename='output/'+ name
    i=1
    while os.path.exists(basename+"_"+"{:03d}".format(i)+'.out'):
        i += 1
    with open(basename+str(i),'w') as fh:
        fh.write( content.as_latex() )
 
[NbConvertApp] output: status
[NbConvertApp] output: execute_input
[NbConvertApp] output: status
[NbConvertApp] Executing cell:
for wipo_sector in wipo_sectors:
    print(wipo_sector)
#     print(df[df.wipo_sector_id==wipo_sector].head())

    print("\n")
    
    myX = df[df.wipo_sector_id==wipo_sector].as_matrix(columns=chosenColumns)
    myY = df[df.wipo_sector_id==wipo_sector].as_matrix(columns=['pagerank'])

    x = sm.add_constant(myX)
    model = sm.OLS(myY, x)
    results = model.fit()
    export_table(results.summary(yname="PageRank", xname=chosenColumns, title="OLS of WIPO: "+wipo_sector ), "wipo_"+wipo_sector)
#     results.summary()
[NbConvertApp] output: status
[NbConvertApp] output: execute_input
[NbConvertApp] output: stream
[NbConvertApp] output: stream
[NbConvertApp] output: stream
[NbConvertApp] output: stream
[NbConvertApp] output: stream
[NbConvertApp] output: status
[NbConvertApp] Executing cell:
# myX = df.as_matrix(columns=chosenColumns)

# myY = df.as_matrix(columns=['parent_citation'])

# xTrain, xTest, yTrain, yTest = train_test_split(myX, myY, train_size=0.7, random_state=3) 
# testSize = yTest.shape[0]
# trainSize = yTrain.shape[0]
# namesList, errorList = best_num_attr(myX, xTrain, xTest, yTrain, yTest, chosenColumns, regtype='linear')
[NbConvertApp] output: status
[NbConvertApp] output: execute_input
[NbConvertApp] output: status
[NbConvertApp] Executing cell:
# myX = df.as_matrix(columns=chosenColumns)
# myY = df.as_matrix(columns=['pagerank'])

# xTrain, xTest, yTrain, yTest = train_test_split(myX, myY, train_size=0.7, random_state=3) 
# testSize = yTest.shape[0]
# trainSize = yTrain.shape[0]
[NbConvertApp] output: status
[NbConvertApp] output: execute_input
[NbConvertApp] output: status
[NbConvertApp] Executing cell:
# namesList, errorList = best_num_attr(myX, xTrain, xTest, yTrain, yTest, chosenColumns, regtype='linear')
[NbConvertApp] output: status
[NbConvertApp] output: execute_input
[NbConvertApp] output: status
[NbConvertApp] Executing cell:
# myX = df.as_matrix(columns=chosenColumns)
# myY = df.as_matrix(columns=['katz'])

# xTrain, xTest, yTrain, yTest = train_test_split(myX, myY, train_size=0.7, random_state=3) 
# testSize = yTest.shape[0]
# trainSize = yTrain.shape[0]
[NbConvertApp] output: status
[NbConvertApp] output: execute_input
[NbConvertApp] output: status
[NbConvertApp] Executing cell:
# namesList, errorList = best_num_attr(myX, xTrain, xTest, yTrain, yTest, chosenColumns, regtype='linear')
[NbConvertApp] output: status
[NbConvertApp] output: execute_input
[NbConvertApp] output: status
[NbConvertApp] Executing cell:
# myX = df.as_matrix(columns=chosenColumns)
# myY = df.as_matrix(columns=['eigen'])

# xTrain, xTest, yTrain, yTest = train_test_split(myX, myY, train_size=0.7, random_state=3) 
# testSize = yTest.shape[0]
# trainSize = yTrain.shape[0]
[NbConvertApp] output: status
[NbConvertApp] output: execute_input
[NbConvertApp] output: status
[NbConvertApp] Executing cell:
# namesList, errorList = best_num_attr(myX, xTrain, xTest, yTrain, yTest, chosenColumns, regtype='linear')
[NbConvertApp] output: status
[NbConvertApp] output: execute_input
[NbConvertApp] output: status
[NbConvertApp] Executing cell:
# This selector does not work because almost every attribute is p-value significant

# selector = SelectKBest(f_classif, k=4) #initialize 
# selector.fit(myX, myY) #fit
# scores = -np.log10(selector.pvalues_) #transform pvalues (why?)
# scores /= scores.max() #normalize 
# plt.bar(myX - .45, scores, width=.2,
#         label=r'Univariate score ($-Log(p_{value})$)', color='darkorange',
#         edgecolor='black')
[NbConvertApp] output: status
[NbConvertApp] output: execute_input
[NbConvertApp] output: status
[NbConvertApp] Executing cell:
# nbayes(xTrain, yTrain, xTest, yTest)
[NbConvertApp] output: status
[NbConvertApp] output: execute_input
[NbConvertApp] output: status
[NbConvertApp] Executing cell:
# df.dropna()
[NbConvertApp] output: status
[NbConvertApp] output: execute_input
[NbConvertApp] output: status
[NbConvertApp] Executing cell:
# #Let's do something else
# #Change the DV 

# myX = df.as_matrix(columns=chosenColumns)
# myY = df.as_matrix(columns=['parent_back_citation'])

# xTrain, xTest, yTrain, yTest = train_test_split(myX, myY, train_size=0.7, random_state=3) 
[NbConvertApp] output: status
[NbConvertApp] output: execute_input
[NbConvertApp] output: status
[NbConvertApp] Executing cell:
# nbayes(xTrain, yTrain, xTest, yTest)
[NbConvertApp] output: status
[NbConvertApp] output: execute_input
[NbConvertApp] output: status
[NbConvertApp] Executing cell:
# df.parent_back_citation.boxplot()
[NbConvertApp] output: status
[NbConvertApp] output: execute_input
[NbConvertApp] output: status
[NbConvertApp] Executing cell:
#and graphs of back citation in time
[NbConvertApp] output: status
[NbConvertApp] output: execute_input
[NbConvertApp] output: status
[NbConvertApp] Executing cell:
# for i in classifications:
#     rank=df.groupby(i).count().iloc[:,2].sort_values(ascending=False).reset_index().set_index(i)
#     description=df_class[df_class['class']==i].set_index('id')
#     display(rank.join(description))
[NbConvertApp] output: status
[NbConvertApp] output: execute_input
[NbConvertApp] output: status
[NbConvertApp] Executing cell:
# for i in obj_cols:
#     if i.isin(classifications):
#         df.join(df.groupby(i).count().iloc[:,2].sort_values(ascending=False)
# #     display.display(df.pivot_table(values=df.reset_index().id, index=i, columns='decade', aggfunc='count', fill_value=0, margins=False, dropna=True))
#     print(i)
#     display.display(df.groupby(i).count().iloc[:,2].sort_values(ascending=False))
[NbConvertApp] output: status
[NbConvertApp] output: execute_input
[NbConvertApp] output: status
[NbConvertApp] Executing cell:
# def cramers_v(x, y):
#     confusion_matrix = pd.crosstab(x,y)
#     chi2 = ss.chi2_contingency(confusion_matrix)[0]
#     n = confusion_matrix.sum().sum()
#     phi2 = chi2/n
#     r,k = confusion_matrix.shape
#     phi2corr = max(0, phi2-((k-1)*(r-1))/(n-1))
#     rcorr = r-((r-1)**2)/(n-1)
#     kcorr = k-((k-1)**2)/(n-1)
#     return np.sqrt(phi2corr/min((kcorr-1),(rcorr-1)))
[NbConvertApp] output: status
[NbConvertApp] output: execute_input
[NbConvertApp] output: status
[NbConvertApp] Executing cell:
# all variables
# dtypes={'id':object, 'type':object, 'kind':object, 'num_claims':float, 'cit_received':float, 'cit_made':float,
#        'cit_received_delay':float, 'cit_made_delay':float, 'parent_citation':float,
#        'originality':float, 'generality':float, 'wipo_field_id':object, 'ipcr_section':object,
#        'cpc_section_id':object,'nber_category_id':object,'uspc_mainclass_id':object}
[NbConvertApp] output: status
[NbConvertApp] output: execute_input
[NbConvertApp] output: status
[NbConvertApp] Applying preprocessor: coalesce_streams
[NbConvertApp] Writing 48435 bytes to external_knowledge.ipynb
Sun Aug 23 20:47:16 EDT 2020
