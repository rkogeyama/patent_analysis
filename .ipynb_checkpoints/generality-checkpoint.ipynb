{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nScript to calculate generality and  application data\\nGenerality: how diverse is the impact of a patent\\nThis is done by calculating the herfindal index of citing patents\\n\\nNov 19th, 2020\\nThis version runs in \\nCPU times: user 31min 5s, sys: 2min 56s, total: 34min 2s\\nWall time: 37min 12s\\nNow I add modifications to allow calculation of originality\\nTo do that, I match classification to patent_id and change the grouping to citation_id\\nI think it may be best to change the index to patent_id upfront, then only create an if condition in the grouping section.\\n\\nFeb 7th, 2020\\nWhile the previous version is already up, I want to improve the script\\nI want to make calculations with only one script\\nAlso, I want to compare different class systems\\nthis version took 10 minutes - i am moving the old version away\\n\\nJan 16th, 2020\\nDue to performance problems in the HPC, this script was divided in two, the script following this is generality_2\\ngenerality > 1 is not an error, but a consequence of adopting WIPO\\nit seems that the original calculation had only one class per patent\\nWIPO provides multiple classes - so when you divide by the total number of citations, \\n you do not have the proportion of classes cited anymore\\nto correct this issue, I can calculate Generality and Originality based only on the first WIPO class\\n\\nJan 13th, 2020\\nScript is running but there are two major issues\\n- there should not exist generality > 1 , so there is an error in calculation\\n- too many NANs (about 400k), but I will tackle this issue in 'too_many_nans.ipynb'\\n\\nto tackle the first problem, I'll begin by creating a subset of the database\\nto do that, I'll use USPTO classification system\\n\""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " \"\"\"\n",
    " Script to calculate generality and  application data\n",
    " Generality: how diverse is the impact of a patent\n",
    " This is done by calculating the herfindal index of citing patents\n",
    "\n",
    " Nov 19th, 2020\n",
    " This version runs in \n",
    " CPU times: user 31min 5s, sys: 2min 56s, total: 34min 2s\n",
    " Wall time: 37min 12s\n",
    " Now I add modifications to allow calculation of originality\n",
    " To do that, I match classification to patent_id and change the grouping to citation_id\n",
    " I think it may be best to change the index to patent_id upfront, then only create an if condition in the grouping section.\n",
    " \n",
    " Feb 7th, 2020\n",
    " While the previous version is already up, I want to improve the script\n",
    " I want to make calculations with only one script\n",
    " Also, I want to compare different class systems\n",
    " this version took 10 minutes - i am moving the old version away\n",
    "\n",
    " Jan 16th, 2020\n",
    " Due to performance problems in the HPC, this script was divided in two, the script following this is generality_2\n",
    " generality > 1 is not an error, but a consequence of adopting WIPO\n",
    " it seems that the original calculation had only one class per patent\n",
    " WIPO provides multiple classes - so when you divide by the total number of citations, \n",
    "  you do not have the proportion of classes cited anymore\n",
    " to correct this issue, I can calculate Generality and Originality based only on the first WIPO class\n",
    "\n",
    " Jan 13th, 2020\n",
    " Script is running but there are two major issues\n",
    " - there should not exist generality > 1 , so there is an error in calculation\n",
    " - too many NANs (about 400k), but I will tackle this issue in 'too_many_nans.ipynb'\n",
    "\n",
    " to tackle the first problem, I'll begin by creating a subset of the database\n",
    " to do that, I'll use USPTO classification system\n",
    " \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "import dask.dataframe as dd\n",
    "from dask.delayed import delayed\n",
    "\n",
    "import glob\n",
    "import graphviz\n",
    "\n",
    "file_list=glob.glob(\"data/citation/*\")\n",
    "classification = 'data/wipo.parquet.gz'\n",
    "\n",
    "class_df=pd.read_parquet(classification, columns=['wipo_sector_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_class(df, class_df):\n",
    "    df=df.merge(class_df, how='inner', right_index=True, left_index=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this function reads the custom-made wipo groups and prepare the dict for aggregation\n",
    "#the aggregation features one count and sums for all classes\n",
    "def wipo_categories():\n",
    "    file_classes = 'data/classes.csv.gz'\n",
    "    classes=pd.read_csv(file_classes, compression='gzip')\n",
    "    classes=classes[classes['system']=='wipo_field_id'].sector_title.unique().tolist()\n",
    "    aggregation={}\n",
    "    for i, element in enumerate(classes):\n",
    "        if i==0:\n",
    "                aggregation[element]=['count','sum']\n",
    "        else:\n",
    "            aggregation[element]='sum'\n",
    "    return aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test gen_orig(df)\n",
    "def test_gen_orig():\n",
    "    df1 = pd.DataFrame(np.random.randint(0,5,size=(10, 1)), columns=['wipo_sector_id'])\n",
    "    df2 = pd.DataFrame(np.random.randint(0,5,size=(10, 1)), columns=['wipo_sector_id'])\n",
    "    df3 = pd.DataFrame(np.random.randint(0,5,size=(10, 1)), columns=['wipo_sector_id'])\n",
    "    df=pd.concat([df1, df2, df3])\n",
    "    print(df.values)\n",
    "#     print(gen_orig(df))\n",
    "    return gen_orig(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# receives a table made by prepare_df and calculates 1-herfindal\n",
    "# this function calculates generality based on Herfindal index\n",
    "# Herfindal is the sum of the squares divided by the square of the sum of citations\n",
    "# it is a measure of concentration\n",
    "\n",
    "# i separate from prepare_df() to make testing easier\n",
    "def gen_orig(df, originality=False):\n",
    "    df=pd.get_dummies(df, columns=['wipo_sector_id'])\n",
    "    if originality:\n",
    "        df=df.groupby(df['citation_id']).sum()\n",
    "        column='originality'\n",
    "    else:\n",
    "        df=df.groupby(df['patent_id']).sum()\n",
    "        column='generality'\n",
    "    df[column] = 1-(df.apply(np.square).apply(np.sum, axis='columns')/df.apply(np.sum, axis='columns').apply(np.square))\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# patent_id is the citing patent\n",
    "# citation_id (the index) is the cited patent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generality(originality=False):\n",
    "    if originality:\n",
    "        dfs = [dd.read_parquet(f, columns=['patent_id']).reset_index().set_index('patent_id') for f in file_list[0:2]]\n",
    "        dst= 'data/originality.parquet.gz'\n",
    "    else:\n",
    "        dfs = [dd.read_parquet(f, columns=['patent_id']) for f in file_list]\n",
    "        dst= 'data/generality.parquet.gz'\n",
    "\n",
    "    \n",
    "    for i,dfx in enumerate(dfs):\n",
    "        dfx=join_class(dfx, class_df) #merge into indexes means merging class with citation_id - thus generality\n",
    "        if i==0:\n",
    "            df=dfx\n",
    "        else:\n",
    "            df=dd.concat([df,dfx])\n",
    "\n",
    "    df=df.repartition(npartitions=8)\n",
    "    df=delayed(gen_orig)(df,originality)\n",
    "\n",
    "    return df, dst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df, dst=generality(originality=True)\n",
    "# df.visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# df=df.compute()\n",
    "# df.to_parquet(dst, compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-f625bbf9c7f2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdst\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgenerality\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvisualize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-ace6a3d90d7b>\u001b[0m in \u001b[0;36mgenerality\u001b[0;34m(originality)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdfx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdfs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mdfx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mjoin_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdfx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_df\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#merge into indexes means merging class with citation_id - thus generality\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0mdf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdfx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-75d883930d5e>\u001b[0m in \u001b[0;36mjoin_class\u001b[0;34m(df, class_df)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mjoin_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mdf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclass_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'inner'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mright_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mleft_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/dask/dataframe/core.py\u001b[0m in \u001b[0;36mmerge\u001b[0;34m(self, right, how, on, left_on, right_on, left_index, right_index, suffixes, indicator, npartitions, shuffle)\u001b[0m\n\u001b[1;32m   4116\u001b[0m             \u001b[0mnpartitions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnpartitions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4117\u001b[0m             \u001b[0mindicator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindicator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4118\u001b[0;31m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4119\u001b[0m         )\n\u001b[1;32m   4120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/dask/dataframe/multi.py\u001b[0m in \u001b[0;36mmerge\u001b[0;34m(left, right, how, on, left_on, right_on, left_index, right_index, suffixes, indicator, npartitions, shuffle, max_branch)\u001b[0m\n\u001b[1;32m    465\u001b[0m             \u001b[0mright_on\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    466\u001b[0m             \u001b[0mright_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 467\u001b[0;31m         \u001b[0mright\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfrom_pandas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mright\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnpartitions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# turn into DataFrame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    468\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    469\u001b[0m     \u001b[0;31m# Both sides are now dd.DataFrame or dd.Series objects\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/dask/dataframe/io/io.py\u001b[0m in \u001b[0;36mfrom_pandas\u001b[0;34m(data, npartitions, chunksize, sort, name)\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msort\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_monotonic_increasing\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 214\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mascending\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    215\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msort\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m         divisions, locations = sorted_division_locations(\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36msort_index\u001b[0;34m(self, axis, level, ascending, inplace, kind, na_position, sort_remaining, ignore_index, key)\u001b[0m\n\u001b[1;32m   5458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5459\u001b[0m             indexer = nargsort(\n\u001b[0;32m-> 5460\u001b[0;31m                 \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkind\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkind\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mascending\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mascending\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mna_position\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mna_position\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5461\u001b[0m             )\n\u001b[1;32m   5462\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/sorting.py\u001b[0m in \u001b[0;36mnargsort\u001b[0;34m(items, kind, ascending, na_position, key)\u001b[0m\n\u001b[1;32m    299\u001b[0m         \u001b[0mnon_nans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnon_nans\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m         \u001b[0mnon_nan_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnon_nan_idx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 301\u001b[0;31m     \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnon_nan_idx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnon_nans\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margsort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkind\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkind\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    302\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mascending\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "df, dst=generality()\n",
    "df.visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df=df.compute()\n",
    "df.to_parquet(dst, compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
