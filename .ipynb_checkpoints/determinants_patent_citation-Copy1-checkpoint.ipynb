{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Script to evaluate the determinants of patent citation \n",
    "#Renato Kogeyama\n",
    "\n",
    "# Mar 27, 2020\n",
    "# features by classification (WIPO)\n",
    "\n",
    "# Mar 16, 2020\n",
    "# Introducing centrality measures as dv\n",
    "\n",
    "# Feb 04, 2020\n",
    "# to set labels in heatmap keyword:xticklabels\n",
    "# for ex.\n",
    "# sns.heatmap(globalWarming_df, xticklabels = np.arange(0,15))\n",
    "# to be implemented later\n",
    "# another alternative is to substitute the values in the dataset and convert columns to categories\n",
    "# to understand the impact, i should run some test\n",
    "# however i am focusing now in calculate Corredoira's 2015 and Nemet & Johnson 2012\n",
    "\n",
    "# Feb 03, 2020\n",
    "# version backed up as _old\n",
    "\n",
    "# Feb 02, 2020\n",
    "# the best way to deal with the classification names is to use a dictionary\n",
    "# this avoid charging memory with the strings\n",
    "# However, WIPO is organized differently than the other systems\n",
    "# I'll update the wipo code to uniformize the behavior in this script\n",
    "# I am creating a code that reflects the first level of classification \n",
    "\n",
    "# Feb 01, 2020\n",
    "# Introduction of categorical graphs: barplot and heatmap\n",
    "# heatmap is not the real deal, its a simplification\n",
    "# the real deal would be the correlation table - there is a suggestion based on cramer, \n",
    "    # but implementation was not ready\n",
    "# graphs exported and google docs updated\n",
    "# next step: update cit_tree to reflect Corredoira's 205 Influence measure\n",
    "# plot a network graph: https://plot.ly/python/network-graphs/\n",
    "# reproduce 2012 Nemet and Johnson with other class systems\n",
    "# correct bias in generality and originality (multiply for N/N-1)\n",
    "\n",
    "# Jan 21, 2020\n",
    "# Classifications added\n",
    "# Code reorganized - much faster now\n",
    "# Still missing the update of applications to the grant number\n",
    "# I should provide now descriptive statistics on all variables\n",
    "\n",
    "# Jan 21, 2020\n",
    "# The current data does not have Class\n",
    "# I should go back and get this info - but there are too many scripts now and\n",
    "#   I should reorganize them before moving forward\n",
    "# I should also include the patent publication date - to control for the policy changes\n",
    "# In the citation file, I should change application number for grant when possible \n",
    "#   This will improve realiability of all measures related to citation\n",
    "# Introduce classifications\n",
    "\n",
    "# Jan 18, 2020\n",
    "# Variables calculated\n",
    "# Generality, average delay, forward and backward citations, cumulative citation (cit_tree)\n",
    "# Still missing originality\n",
    "# the file with variables that are used in this script should get a name independent from the date\n",
    "\n",
    "\n",
    "#Miami, December 24th, 2019\n",
    "# Prof. Rafael Corredoira suggested:\n",
    "# - Inclusion of a tree of citations\n",
    "#   To track back the source of citations. This is information is not given by direct count of citations.\n",
    "# - Consider policy changes in the way patents are cited\n",
    "#   Policy changes in 2000 changed the time frame of citation, and 2010 partially moved citation to applications\n",
    "# - Track classification changes \n",
    "#   The original classification system in USPTO changed from a technical based to a market based classification system\n",
    "#   See if there is an impact\n",
    "# - Consider a text analysis of the claims\n",
    "#   Classification is based on the claims but it is not clear how many claims are related to each classification category\n",
    "# - Include moderation effect from classification\n",
    "#   Citations patterns may change across industries, so some effects may disappear if industry is not accounted for.\n",
    "\n",
    "# In summary, his ideas help increase structure of the current work\n",
    "\n",
    "\n",
    "#Syracuse, December 3rd, 2019\n",
    "\n",
    "#The original script is getting too complex\n",
    "#There was many tentative scripts to play with data\n",
    "#Here I am writing a script to show the relevance of variables to patent citation\n",
    "\n",
    "#11-12-2019\n",
    "#Introducing normalization\n",
    "\n",
    "#10-11-2019\n",
    "#I introduced log backward citation, what corrects for very dispersed results\n",
    "#but the major problem is that few patents receive citations\n",
    "#bring back binary output\n",
    "\n",
    "#10-10-2019\n",
    "#Added graphics and new distributions\n",
    "\n",
    "#10-03-2019\n",
    "#I rewrote the citation data to clean the strings\n",
    "\n",
    "#09-15-2019\n",
    "#O naive bayes tem algum problema com distribuicoes desbalanceadas\n",
    "#o scikit learn tem um modulo que corrige count distributions com muitos zeros, o complementNB\n",
    "#porem este nao esta disponivel na atual versao disponibilizada no HPC da FIU\n",
    "\n",
    "#09-10-2019\n",
    "#o trabalho pede uma abordagem mais sistematica e cuidadosa\n",
    "#estou agrupando o codigo antigo comentado e vou comecar um novo codigo\n",
    "\n",
    "#09-27-2019\n",
    "#I am renaming citation as forward citation and backward citation\n",
    "\n",
    "#09-17-2018\n",
    "\n",
    "#Alto uso de memoria - rodar no Amazon AWS \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named pandas",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-bab5cd55a294>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mIPython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mdisplay\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mseaborn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named pandas"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import IPython.display as display\n",
    "import seaborn as sns\n",
    "          \n",
    "import itertools\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn import linear_model, datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn import naive_bayes\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "import scipy.stats as ss\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from math import sqrt\n",
    "\n",
    "import sys\n",
    "sys.path.append('/home/rkogeyam/scripts/')\n",
    "sys.path.append('scripts/')\n",
    "\n",
    "from determinants_scripts import classes\n",
    "\n",
    "from plotbar import plotbar\n",
    "from plot_heat import heatmap\n",
    "\n",
    "\n",
    "from best_num_attr import best_num_attr\n",
    "from xattrSelect import xattrSelect\n",
    "from sampler import sampler\n",
    "from normalize import normalize\n",
    "from nbayes import nbayes\n",
    "\n",
    "import gzip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# print('The scikit-learn version is {}.'.format(sklearn.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "latex='data/results.tex'\n",
    "dataset=gzip.open('data/dataset.csv.gz', 'rt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# dataset=source.open('dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "sns.set()\n",
    "sns.set_palette(sns.cubehelix_palette(8))\n",
    "# pd.options.display.float_format = '{:,.2f}'.format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dtypes={'id':object,'type':object, 'kind':object, 'num_claims':float, 'cit_received':float, 'cit_made':float,\n",
    "       'cit_received_delay':float, 'cit_made_delay':float, 'parent_citation':float,\n",
    "       'originality':float, 'generality':float, 'wipo_sector_id':object, 'ipcr_section':object,\n",
    "       'ipcr_ipc_class':object, 'ipcr_subclass':object, 'cpc_section_id':object,\n",
    "       'cpc_subsection_id':object, 'cpc_group_id':object, 'nber_category_id':object,\n",
    "       'nber_subcategory_id':object, 'uspc_mainclass_id':object, 'uspc_subclass_id':object, 'eigen':float, 'pagerank':float, 'katz':float}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # only main classes (exclude uspc)\n",
    "\n",
    "# usecols=['id', 'type', 'date', 'kind', 'num_claims', 'cit_received', 'cit_made',\n",
    "#          'cit_received_delay', 'cit_made_delay', 'parent_citation',\n",
    "#          'originality', 'generality', 'wipo_sector_id', 'ipcr_section', \n",
    "#          'cpc_section_id', 'nber_category_id', 'eigen', 'pagerank', 'katz']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# only WIPO class system, exclude type and kind\n",
    "\n",
    "usecols=['id', 'date', 'num_claims', 'cit_received', 'cit_made',\n",
    "         'cit_received_delay', 'cit_made_delay', 'parent_citation',\n",
    "         'originality', 'generality', 'wipo_sector_id', 'eigen', 'pagerank', 'katz']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df=pd.read_csv(dataset, dtype=dtypes, usecols=usecols, parse_dates=['date'], index_col='id')\n",
    "\n",
    "# df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['year']=df.date.dt.year\n",
    "\n",
    "df['decade']=df.date.dt.year//10*10\n",
    "df['decade'] =df['decade'].apply(lambda x: int(x) if str(x) != 'nan' else np.nan)\n",
    "decades=list(df.decade.unique())\n",
    "# decades = [int(x) for x in decades if str(x) != 'nan']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wipo_sector_list=list(df.wipo_sector_id.unique())\n",
    "\n",
    "# obj_cols=list(df.select_dtypes(include=[object]).columns.values)\n",
    "# obj_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_cols=list(df.select_dtypes(include=[np.number]).columns.values)\n",
    "num_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Analysis\n",
    "\n",
    "### Descriptive "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "descriptive=df.describe(include=[np.number]).loc[['count','mean','std','min','max']].append(df[num_cols].isnull().sum().rename('isnull'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "descriptive.apply(lambda x: x.apply('{:,.2f}'.format)).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.describe(include=[np.object])#.append(df[np.object].isnull().sum().rename('isnull')).transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Barplots and Heatmaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # barplot\n",
    "# # as of 02.03.20, working\n",
    "\n",
    "# for i in obj_cols:\n",
    "#     plotbar(i, df, classes)\n",
    "\n",
    "# # barplot with decades\n",
    "# for i in obj_cols:\n",
    "#     plotbar(i, df, classes,decade=True)\n",
    "\n",
    "# # barplot with decades and inverted axis\n",
    "# for i in obj_cols:\n",
    "#     plotbar(i, df, classes,decade=True, decade_x=True)\n",
    "\n",
    "# # heatmaps all periods\n",
    "# for double in list(itertools.combinations(obj_cols, 2)):\n",
    "#     heatmap(df[double[0]], df[double[1]]) \n",
    "\n",
    "# # print heatmaps per decade\n",
    "# for decade in decades:\n",
    "#     df_dec=df[df['decade']==decade]\n",
    "#     for double in list(itertools.combinations(obj_cols, 2)):\n",
    "#         heatmap(df_dec[double[0]], df_dec[double[1]], decade) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# #histograms\n",
    "# #could improve cutting off outliers\n",
    "# for variable in num_cols:\n",
    "#     ax=df[variable].hist()\n",
    "#     ax.set_title('Histogram '+ variable.title()+'\\n')\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trends and Boxplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#iterate over numerical variables\n",
    "\n",
    "num_cols.remove('decade')\n",
    "num_cols.remove('year')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# for variable in num_cols:\n",
    "    \n",
    "#     title=variable.replace('_', ' ')\n",
    "#     fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(14, 6))\n",
    "\n",
    "#     axes[0] = df.groupby('year').mean().plot(y=variable, ax=axes[0])\n",
    "#     evl_title='Evolution of '+ title +'\\n'\n",
    "#     axes[0].set_title(evl_title)\n",
    "#     axes[0].set_ylim(bottom=0)\n",
    "    \n",
    "#     axes[1] = sns.boxplot(x='decade', y=variable, data=df)\n",
    "\n",
    "#     box_title='Dispersion of '+ title +'\\n'\n",
    "#     axes[1].set_title(box_title)\n",
    "#     axes[1].set_ylim(bottom=0)\n",
    "#     axes[1].set_ylabel(\"\")\n",
    "    \n",
    "#     filename='./img/evol_dispersion_'+variable.lower()+'.png'  \n",
    "#     plt.savefig(filename) \n",
    "#     plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# the generality data on the 2010's is too concentrated around 0\n",
    "# to check, I draw this hist to understand what is happening\n",
    "# it could be an effect of truncation - generality increases with forward citation\n",
    "\n",
    "# df[df['decade']==2010]['generality'].hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# normalization\n",
    "df=normalize(df.dropna())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# #maybe nb fit does not accept nomalized data, so i using data without normalize\n",
    "# #but in that case, i have to transform the categorical variables\n",
    "\n",
    "# obj_cols=list(df.select_dtypes(include=[object]).columns.values)\n",
    "\n",
    "# for col in obj_cols:\n",
    "#     df[col] = df[col].astype('category')\n",
    "\n",
    "# df=pd.get_dummies(df, columns=obj_cols, prefix=obj_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# len(chosenColumns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# List of IVs\n",
    "chosenColumns=df.columns.values.tolist()\n",
    "chosenColumns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "chosenColumns.remove('date')\n",
    "\n",
    "dv_list=['cit_received', 'parent_citation', 'katz', 'eigen', 'pagerank']\n",
    "\n",
    "for element in dv_list:\n",
    "    chosenColumns.remove(element)\n",
    "\n",
    "wipo=['wipo_sector_id_0',\n",
    "     'wipo_sector_id_1',\n",
    "     'wipo_sector_id_2',\n",
    "     'wipo_sector_id_3',\n",
    "     'wipo_sector_id_4']\n",
    "\n",
    "for element in wipo:\n",
    "    chosenColumns.remove(element)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List of best features by classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# for classification in wipo:\n",
    "#     for dv in dv_list:\n",
    "#         myX = df[df[classification]==1].as_matrix(columns=chosenColumns)\n",
    "#         myY = df[df[classification]==1].as_matrix(columns=[dv])\n",
    "\n",
    "#         print('Classification: {}'.format(classification))\n",
    "#         print('Dependent Variable: {}'.format(dv))\n",
    "#         xTrain, xTest, yTrain, yTest = train_test_split(myX, myY, train_size=0.7, random_state=3) \n",
    "#         testSize = yTest.shape[0]\n",
    "#         trainSize = yTrain.shape[0]\n",
    "#         namesList, errorList = best_num_attr(myX, xTrain, xTest, yTrain, yTest, chosenColumns, regtype='linear')\n",
    "#         print('\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# for classification in wipo:\n",
    "#     for dv in dv_list:\n",
    "#         myX = df[df[classification]==1].as_matrix(columns=chosenColumns)\n",
    "#         myY = df[df[classification]==1].as_matrix(columns=dv)\n",
    "#         print('Classification: {}'.format(classification))\n",
    "#         print('Dependent Variable: {}'.format(dv))\n",
    "#         nbayes(xTrain, yTrain, xTest, yTest)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# #Bayes\n",
    "# for dv in dv_list:\n",
    "#     myX = df.as_matrix(columns=chosenColumns)\n",
    "#     myY = df.as_matrix(columns=[dv])\n",
    "#     print('Dependent Variable: {}'.format(dv))\n",
    "\n",
    "#     xTrain, xTest, yTrain, yTest = train_test_split(myX, myY, train_size=0.7, random_state=3) \n",
    "#     nbayes(xTrain, yTrain, xTest, yTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "myX = df.as_matrix(columns=chosenColumns)\n",
    "myY = df.as_matrix(columns=['cit_received'])\n",
    "\n",
    "xTrain, xTest, yTrain, yTest = train_test_split(myX, myY, train_size=0.7, random_state=3) \n",
    "\n",
    "testSize = yTest.shape[0]\n",
    "trainSize = yTrain.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# nb = naive_bayes.MultinomialNB()\n",
    "nb = naive_bayes.GaussianNB()\n",
    "\n",
    "# complementNB adapta o MultinomialNB para datasets muito desbalanceados\n",
    "# porem nao esta disponivel para a versao 0.19 do scikit-learn\n",
    "# nb = naive_bayes.ComplementNB()\n",
    "\n",
    "nb.fit(xTrain, yTrain.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainingPredict = nb.predict(xTrain) # predicting test data\n",
    "\n",
    "yPredNB = nb.predict(xTest) # predicting test data\n",
    "\n",
    "yTrue=[(yPredNB[i] != yTest[i]) for i in range(0, testSize)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # computing error\n",
    "# yTrue=np.array((yPredNB[i] != yTest[i]) for i in range(0, testSize))\n",
    "errorNB = np.sum(yTrue)\n",
    "errorNBPCT = int(100*errorNB/testSize)\n",
    "hitRateNBPCT = 100 - errorNBPCT\n",
    "\n",
    "fpr, tpr, thresholds=roc_curve(yTrain.ravel(), trainingPredict, pos_label=2)\n",
    "print(\"----------Naive Bayes----------\")\n",
    "print(int(errorNB), \"misclassified data out of\", testSize)\n",
    "print(\"Error PCT: \",errorNBPCT,'%')\n",
    "print(\"Hit Rate:  \",hitRateNBPCT,'%')\n",
    "\n",
    "roc_auc=auc(fpr, tpr)\n",
    "\n",
    "print('AUC for in-sample ROC curve: %f' % roc_auc)\n",
    "\n",
    "pl.clf()\n",
    "pl.plot(fpr, tpr, label='ROC curve (area=%0.2f)' % roc_auc)\n",
    "pl.plot([0,1], [0,1], 'k-')\n",
    "pl.xlim([0.0,1.0])\n",
    "pl.xlabel('False Positive Rate')\n",
    "pl.ylabel('True Positive Rate')\n",
    "pl.title('In sample ROC')\n",
    "pl.legend(loc='lower right')\n",
    "pl.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# df.parent_back_citation.boxplot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#and graphs of back citation in time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# for i in classifications:\n",
    "#     rank=df.groupby(i).count().iloc[:,2].sort_values(ascending=False).reset_index().set_index(i)\n",
    "#     description=df_class[df_class['class']==i].set_index('id')\n",
    "#     display(rank.join(description))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# for i in obj_cols:\n",
    "#     if i.isin(classifications):\n",
    "#         df.join(df.groupby(i).count().iloc[:,2].sort_values(ascending=False)\n",
    "# #     display.display(df.pivot_table(values=df.reset_index().id, index=i, columns='decade', aggfunc='count', fill_value=0, margins=False, dropna=True))\n",
    "#     print(i)\n",
    "#     display.display(df.groupby(i).count().iloc[:,2].sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def cramers_v(x, y):\n",
    "#     confusion_matrix = pd.crosstab(x,y)\n",
    "#     chi2 = ss.chi2_contingency(confusion_matrix)[0]\n",
    "#     n = confusion_matrix.sum().sum()\n",
    "#     phi2 = chi2/n\n",
    "#     r,k = confusion_matrix.shape\n",
    "#     phi2corr = max(0, phi2-((k-1)*(r-1))/(n-1))\n",
    "#     rcorr = r-((r-1)**2)/(n-1)\n",
    "#     kcorr = k-((k-1)**2)/(n-1)\n",
    "#     return np.sqrt(phi2corr/min((kcorr-1),(rcorr-1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# all variables\n",
    "# dtypes={'id':object, 'type':object, 'kind':object, 'num_claims':float, 'cit_received':float, 'cit_made':float,\n",
    "#        'cit_received_delay':float, 'cit_made_delay':float, 'parent_citation':float,\n",
    "#        'originality':float, 'generality':float, 'wipo_field_id':object, 'ipcr_section':object,\n",
    "#        'cpc_section_id':object,'nber_category_id':object,'uspc_mainclass_id':object}"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
